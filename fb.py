# -*- coding: utf-8 -*-
"""income added

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wBHP1eInJUhus1cEaIoQVhxrgur9WAxy
"""

# app.py
# Restaurant White-Space & Opportunity Tool ‚Äì Philadelphia
#
# Run in terminal with:
#   python3 -m pip install streamlit pandas numpy scikit-learn pydeck
#   python3 -m streamlit run app.py

import re
import numpy as np
import pandas as pd
import streamlit as st
import pydeck as pdk

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GroupKFold, cross_val_score


import streamlit as st

# ...

@st.dialog("How this tool works", width="large")
def show_learn_more_dialog():
    st.markdown(
        """
        ### What business problem does this solve?

        Choosing **where** to open a restaurant ‚Äì and **what concept** to open ‚Äì is one of the most expensive
        and risky decisions an operator makes.  
        
        This tool helps you:
        - Avoid guessing based only on ‚Äúgut feel‚Äù or a few drive-bys.  
        - See which **ZIP codes** look most promising for your concept.  
        - See which **concepts** look promising for a specific ZIP you already like.  
        - Spot places where demand is high but existing restaurants are not keeping people happy.

        Think of it as a **site-selection and concept-selection assistant** built on actual data from
        restaurants in Philadelphia.

        ---

        ### Two ways to use the tool

        You can use the app in two modes:

        - **Mode A ‚Äì You already have a concept in mind**  
          Example: ‚ÄúI want to open a mid-priced Mexican place with sit-down service and alcohol.‚Äù  
          - You describe your concept once.  
          - The tool scans all ZIP codes and **ranks the ZIPs** by how attractive they look for *that* concept.

        - **Mode B ‚Äì You already have a ZIP / neighborhood in mind**  
          Example: ‚ÄúI‚Äôm committed to this ZIP. What should I put there?‚Äù  
          - You pick a ZIP.  
          - The tool compares many possible concepts and **ranks the concepts** for that specific ZIP.

        In both cases the ranking is based on the same core idea:  
        > ‚ÄúHow likely is this concept to work **here**, and how crowded is the space already?‚Äù

        ---

        ### How the prediction model works (in plain English)

        Under the hood we use a **random forest model** ‚Äì a type of machine-learning algorithm that combines
        many decision trees. Each tree makes a simple yes/no series of splits (e.g., ‚Äúis this a high-income ZIP?‚Äù,
        ‚Äúis this concept mid-priced?‚Äù, ‚Äúdoes it serve alcohol?‚Äù), and the forest averages across all those trees
        to get a stable prediction.

        What we actually predict is:

        > **The probability that a restaurant will achieve a Yelp rating of at least 4.0‚òÖ.**

        To train the model we use data from existing restaurants in Philadelphia. For each restaurant, the model sees:
        - Cuisine type and price level  
        - Whether there is delivery, takeout, outdoor seating, TV, alcohol type, etc.  
        - The ZIP code and income level of that ZIP  
        - The restaurant‚Äôs **actual Yelp rating**

        We turn the rating into a simple success flag:
        - 1 = success if rating ‚â• 4.0‚òÖ  
        - 0 = otherwise  

        The random forest learns patterns such as:
        > ‚ÄúConcepts like this, in ZIPs like that, usually do (or do not) reach 4‚òÖ.‚Äù

        Once trained, we can feed in **any hypothetical (concept, ZIP)** and the model outputs a number between 0 and 1:
        - This number is **P(success)** ‚Äì its estimate of  
          > ‚ÄúHow likely is this concept to reach 4‚òÖ or better in this ZIP?‚Äù

        That **P(success)** is the core input into the Opportunity Score.

        ---

        ### How the Opportunity Score works

        A high success probability is good, but it‚Äôs not the whole story, so we add another component to our final Opportunity Score.
        If a ZIP is **already packed** with your cuisine, it‚Äôs harder to stand out.

        So for each (concept, ZIP) we also measure **market saturation**:
        - *Market saturation* ‚âà the share of restaurants in that ZIP that are already in the same cuisine segment.  
          - Example: if 40% of restaurants in a ZIP are pizza places, the saturation for pizza there is 0.40.

        We then combine:
        - **P(success)** from the random forest = predicted chance of getting ‚â• 4‚òÖ  
        - **(1 ‚àí saturation)** = ‚Äúhow much room is left‚Äù for that cuisine in the ZIP  

        The **Opportunity Score** is:

        > `Opportunity Score = P(success) √ó (1 ‚àí saturation)`

        Intuition:
        - High **P(success)** + low **saturation**  ‚Üí **great opportunity**  
        - High **P(success)** but high **saturation** ‚Üí looks good, but very crowded  
        - Low **P(success)** ‚Üí risky, even if there isn‚Äôt much competition

        That‚Äôs the number the app uses to **rank** ZIPs in Mode A and **rank** concepts in Mode B.

        ---

        ### Demand‚ÄìDissatisfaction Index (DDI) ‚Äì what it is and why it‚Äôs separate

        We also compute a **Demand‚ÄìDissatisfaction Index (DDI)** for each ZIP √ó cuisine combination:

        > `DDI = Total number of reviews √ó max(0, 4 ‚àí average rating)`

        Plain-English meaning:
        - Lots of reviews ‚Üí **strong demand**; people are clearly using and talking about these places.  
        - Low average rating ‚Üí customers are **not fully satisfied** with the options they have.  

        So a **high DDI** signals:
        > ‚ÄúMany people are going out for this type of food here,  
        > but they‚Äôre not very happy with what they‚Äôre getting.‚Äù

        That‚Äôs a classic sign of **room in the market to do better**.

        #### Why DDI is *not* inside the Opportunity Score

        We keep DDI **separate** from the Opportunity Score on purpose:

        - Review counts are heavily affected by **restaurant age** ‚Äì older places have had more time
          to accumulate reviews, even if demand is not especially strong.  
        - If we mixed DDI directly into the Opportunity Score, we would risk **biasing** the rankings toward
          older restaurants and long-established areas, not just truly high-demand areas.

        Instead, we treat DDI as a **supplementary diagnostic**:
        - Use the **Opportunity Score** to rank and compare (concept, ZIP) options.  
        - Use **DDI** as an extra lens to spot ‚Äúhigh demand but unhappy customers‚Äù markets
          where a strong new operator could win share.
        """
    )
# -----------------------------
# Session state for model caching
# -----------------------------
if "model_trained" not in st.session_state:
    st.session_state.model_trained = False
    st.session_state.model = None
    st.session_state.auc_mean = None
    st.session_state.auc_std = None


@st.cache_resource
def train_model(df_model, model_features, cv_folds, n_trees, max_depth, numeric, categorical):
    X = df_model[model_features].copy()
    y = df_model["success"].values
    groups = df_model["postal_code"].values

    pre = ColumnTransformer(
        transformers=[
            ("num", "passthrough", numeric),
            ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), categorical),
        ],
        remainder="drop"
    )

    rf_clf = RandomForestClassifier(
        n_estimators=int(n_trees),
        max_depth=None if max_depth == 0 else int(max_depth),
        random_state=42,
        n_jobs=-1
    )

    model = Pipeline(steps=[
        ("prep", pre),
        ("clf", rf_clf)
    ])

    gkf = GroupKFold(n_splits=int(cv_folds))
    auc_scores = cross_val_score(
        model, X, y,
        cv=gkf.split(X, y, groups=groups),
        scoring="roc_auc"
    )
    model.fit(X, y)

    return model, auc_scores.mean(), auc_scores.std()


# -----------------------------
# Page config
# -----------------------------
st.set_page_config(
    page_title="Philly Food and Beverage Concept Opportunity Tool",
    layout="wide"
)

st.title("üçΩÔ∏è Food & Beverage Opportunity & White-Space Tool ‚Äì Philadelphia")
st.caption("Random forest + market saturation to find promising (concept, ZIP) combinations.")

st.markdown("""
<div style='background-color:#e8f2fc; padding:15px; border-radius:8px;'>
<b>Step 1:</b> We auto-train the model on load.<br>
<b>Step 2:</b> Tell us what you already know ‚Äî are you starting from a concept or from a ZIP code?<br>
<b>Step 3:</b> Use the controls below to run the analysis and explore recommended matches.
</div>
""", unsafe_allow_html=True)

if st.button("Learn More"):
    show_learn_more_dialog()

# -----------------------------
# Helper functions
# -----------------------------
CUISINE_TOKENS = [
    "american", "italian", "mexican", "chinese", "japanese", "thai", "indian",
    "korean", "vietnamese", "greek", "mediterranean", "french", "spanish",
    "middle eastern", "latin american", "seafood", "bbq", "pizza",
    "vegan", "vegetarian", "cafe", "pub", "burger", "deli", "steakhouse"
]


# -----------------------------
# ZIP NORMALIZATION
# -----------------------------
def normalize_zip(z):
    """Extract 5-digit ZIP as string if possible."""
    s = str(z)
    m = re.search(r"\d{5}", s)
    return m.group(0) if m else s


# -----------------------------
# BOOLEAN CLEANING
# -----------------------------
def to_bool_series(x: pd.Series) -> pd.Series:
    """Coerce messy string/bool columns into True/False/NaN."""
    if x.dtype == bool:
        return x
    s = x.astype(str).lower().str.strip()
    return s.replace({
        "true": True, "yes": True, "y": True, "1": True,
        "false": False, "no": False, "n": False, "0": False,
        "none": np.nan, "nan": np.nan, "": np.nan
    })


# -----------------------------
# ALCOHOL NORMALIZATION
# -----------------------------
def normalize_alcohol(x: pd.Series) -> pd.Series:
    """Map Yelp alcohol attribute to canonical levels."""
    s = x.fillna("").astype(str).str.lower()
    mapped = np.select(
        [
            s.str.contains("full"),                           # full bar
            s.str.contains("beer") | s.str.contains("wine"),  # beer + wine
            s.eq("none") | s.str.contains("no ")              # none
        ],
        ["full_bar", "beer_and_wine", "none"],
        default="unknown"
    )
    return pd.Series(mapped, index=x.index).astype(str)


# ======================================================
#   üî• REMOVE UNWANTED RESTAURANTS COMPLETELY
# ======================================================
REMOVE_ROWS = [
    "event planning & services",
    "event planning and services",
    "event planning",
    "hotels & travel",
    "hotels",
    "travel",
    "chicken shop",
    "chicken wings",
    "books",
    "arts & entertainment",
    "food",                # removes generic ‚Äúfood‚Äù
    "ethnic food",
    "grocery",
    "health markets",
    "meat shops",
    "personal chefs",
    "restaurants",
    "shopping",
    "comfort food",
    "soul food",
    "specialty food",
    "caterers"
]

# Call this right after reading df:
# df = df[~df["categories"].str.lower().str.contains(remove_pattern, na=False)]

remove_pattern = "|".join([re.escape(r) for r in REMOVE_ROWS])


# ======================================================
#   üîÑ MERGE CATEGORIES INTO UNIFIED CUISINES
# ======================================================
def extract_cuisine_main(categories: pd.Series) -> pd.Series:
    """Extract main cuisine and apply merging rules."""

    cats = categories.fillna("").str.lower()

    MERGE = {
        # Caf√© cluster
        "cafe": "cafe",
        "coffee": "cafe",
        "coffee & tea": "cafe",
        "coffee shop": "cafe",
        "coffee roaster": "cafe",
        "tea": "cafe",
        "bakery": "cafe",
        "bakeries": "cafe",
        "bagel": "cafe",
        "bagels": "cafe",

        # Food stands cluster
        "food court": "food_stands",
        "food stand": "food_stands",
        "food stands": "food_stands",

        # Pub/Bar cluster
        "pub": "pub",
        "bar": "pub",
        "sports bar": "pub",
        "beer": "pub",

        # Japanese cluster
        "sushi": "japanese",
        "sushi bar": "japanese",
        "sushi bars": "japanese",
        "ramen": "japanese",
        "japanese": "japanese"
    }

    def pick(s):
        tokens = [t.strip() for t in s.split(",") if t.strip()]

        # merge rules
        for t in tokens:
            for raw, merged in MERGE.items():
                if raw in t:
                    return merged

        # fallback to CUISINE_TOKENS
        for t in tokens:
            for key in CUISINE_TOKENS:
                if key in t:
                    return key

        return None

    return cats.apply(pick).astype(str)


# -----------------------------
# SEGMENT ID (unchanged)
# -----------------------------
def build_segment_id(cuisine, price=None):
    """Use cuisine only for market saturation segmentation."""
    if cuisine is None:
        return None
    return str(cuisine).strip().lower()


# -----------------------------
# ZIP CENTROIDS (unchanged)
# -----------------------------
def make_zip_centroids(df: pd.DataFrame) -> pd.DataFrame:
    """Compute ZIP centroids from lat/lon."""
    g = df.groupby("postal_code", as_index=False).agg(
        lat=("latitude", "mean"),
        lon=("longitude", "mean"),
        n=("postal_code", "size")
    )
    return g.dropna(subset=["lat", "lon"])


# -----------------------------
# Load data (preloaded, no upload widget)
# -----------------------------
CSV_PATH = r"philly_clean_food_drink_gt10_nas_dropped.csv"  # same folder as app.py

try:
    df = pd.read_csv(CSV_PATH)
except FileNotFoundError:
    st.error(
        f"Could not find data file '{CSV_PATH}'. "
        "Place the CSV in the same folder as app.py or edit CSV_PATH in the code."
    )
    st.stop()


required_cols = [
    "name", "city", "state", "postal_code",
    "latitude", "longitude", "stars", "review_count",
    "categories", "price", "takeout", "delivery",
    "outdoor", "credit", "has_tv", "alcohol"
]

missing = [c for c in required_cols if c not in df.columns]
if missing:
    st.error(f"CSV is missing required columns: {missing}")
    st.stop()

# ADDED INCOME:
# -----------------------------
# Load income table (local CSV using semicolon)
# -----------------------------
df2 = pd.read_csv(
    r"income_table.csv",
    sep=";"
)

# Clean postal codes (make sure they are 5-digit strings)
df2["postal_code"] = df2["postal_code"].astype(str).str.extract(r"(\d{5})")

# Ensure income is numeric
df2["income"] = pd.to_numeric(df2["income"], errors="coerce")

# Clean postal codes in Yelp data as well
df["postal_code"] = df["postal_code"].astype(str).str.extract(r"(\d{5})")

# Merge income table into Yelp data
df = df.merge(df2, on="postal_code", how="left")

# Fill remaining missing income values
df["income"] = df["income"].fillna(df["income"].median())

# For ZIP-level lookup in predictions
income_lookup = df.set_index("postal_code")["income"].to_dict()

# Basic cleaning
# adding changes for income
df["income"] = pd.to_numeric(df["income"], errors="coerce")
df["postal_code"] = df["postal_code"].apply(normalize_zip)
df["price"] = pd.to_numeric(df["price"], errors="coerce")
df["review_count"] = pd.to_numeric(df["review_count"], errors="coerce")

for col in ["takeout", "delivery", "outdoor", "credit", "has_tv"]:
    df[col] = to_bool_series(df[col])

df["alcohol_norm"] = normalize_alcohol(df["alcohol"])
df["cuisine_main"] = extract_cuisine_main(df["categories"])
df = df[df["cuisine_main"].notna()]
df["success"] = (df["stars"] >= 4).astype(int)

# income also added
model_features = [
    "price", "takeout", "delivery", "outdoor",
    "credit", "has_tv", "alcohol_norm",
    "cuisine_main", "postal_code", "income"
]
model_deps = model_features + ["success"]

# Convert income to numeric again (safety)
df["income"] = pd.to_numeric(df["income"], errors="coerce")

# Fill missing income values with median (important: prevents df_model from becoming empty)
df["income"] = df["income"].fillna(df["income"].median())

df_model = df[model_deps].dropna()
n_before = len(df)
n_after = len(df_model)

# ZIP centroids (for map)
zip_geo = make_zip_centroids(df)

# -----------------------------
# Segment & market saturation (count-based)
# -----------------------------
# Ensure single-value columns (remove duplicate columns created by merge)
df_model = df_model.loc[:, ~df_model.columns.duplicated()]

# Build segment_id safely and vectorized
df_model["segment_id"] = df_model["cuisine_main"].astype(str).str.strip().str.lower()

# Market saturation by count
zip_totals = df_model.groupby("postal_code").size().rename("zip_total").reset_index()

seg_counts = (
    df_model.groupby(["postal_code", "segment_id"])
    .size()
    .rename("segment_count")
    .reset_index()
)

market_share_df = seg_counts.merge(zip_totals, on="postal_code", how="left")
market_share_df["market_share"] = market_share_df["segment_count"] / market_share_df["zip_total"]

market_share_lookup = {
    (row["postal_code"], row["segment_id"]): row["market_share"]
    for _, row in market_share_df.iterrows()
}

# -----------------------------
# Demand‚ÄìDissatisfaction diagnostic (heuristic)
# -----------------------------
agg_ddi = (
    df
    .groupby(["postal_code", "cuisine_main"], as_index=False)
    .agg(
        avg_rating=("stars", "mean"),
        total_reviews=("review_count", "sum"),
        n_venues=("postal_code", "size")
    )
)

agg_ddi["dissatisfaction_gap"] = (4 - agg_ddi["avg_rating"]).clip(lower=0)
agg_ddi["ddi"] = agg_ddi["total_reviews"] * agg_ddi["dissatisfaction_gap"]

# Preprocessing feature groups
numeric = ["price", "income"]
categorical = [
    "takeout", "delivery", "outdoor", "credit", "has_tv",
    "alcohol_norm", "cuisine_main", "postal_code"
]

# -----------------------------
# Train Random Forest (success) ‚Äì auto, no sidebar
# -----------------------------
cv_folds, n_trees, max_depth, n_estimators = 5, 300, 10, 200  # unlimited depth

if not st.session_state.model_trained:
    with st.spinner("Training model automatically... please wait ‚è≥"):
        model, auc_mean, auc_std = train_model(
            df_model, model_features, cv_folds, n_trees, max_depth, numeric, categorical
        )
        st.session_state.model = model
        st.session_state.auc_mean = auc_mean
        st.session_state.auc_std = auc_std
        st.session_state.model_trained = True
    st.success("Model trained!")
elif st.session_state.model_trained:
    st.info(
        f"Grouped CV AUC: **{st.session_state.auc_mean:.3f} ¬± {st.session_state.auc_std:.3f}**"
    )

# ZIP choices used in both modes
unique_zips = sorted(
    [z for z in df_model["postal_code"].astype(str).unique() if str(z).lower() != "nan"]
)

# Concept helpers
# -----------------------------
def segment_id_from_concept(cuisine, price=None):
    # Keep the signature but ignore price for market saturation
    return build_segment_id(cuisine)


def lookup_market_share(zip_code, segment_id):
    key = (normalize_zip(zip_code), segment_id)
    return market_share_lookup.get(key, 0.0)

# Food vs drink utility
def classify_food_or_drink(cuisine, alcohol_norm):
    val = str(alcohol_norm).lower()
    cuisine_val = str(cuisine).lower()

    drink_keywords = [
        "bar", "pub", "brewery", "cocktail", "wine", "beer", "cafe", "coffee", "tea"
    ]
    is_drink_cuisine = any(k in cuisine_val for k in drink_keywords)

    if val in {"full_bar", "beer_and_wine"} and is_drink_cuisine:
        return "drink"
    return "food"


def apply_food_drink_filter(df, kind_column, choice):
    if choice == "food_only":
        return df[df[kind_column] == "food"]
    if choice == "drink_only":
        return df[df[kind_column] == "drink"]
    return df


# INCOME ADDED
def predict_success_for_concept(concept, zips):
    data = {
        "price": [],
        "takeout": [],
        "delivery": [],
        "outdoor": [],
        "credit": [],
        "has_tv": [],
        "alcohol_norm": [],
        "cuisine_main": [],
        "postal_code": [],
        "income": []
    }
    for z in zips:
        data["price"].append(concept["price"])
        data["takeout"].append(concept["takeout"])
        data["delivery"].append(concept["delivery"])
        data["outdoor"].append(concept["outdoor"])
        data["credit"].append(concept["credit"])
        data["has_tv"].append(concept["has_tv"])
        data["alcohol_norm"].append(concept["alcohol_norm"])
        data["cuisine_main"].append(concept["cuisine_main"])
        data["postal_code"].append(z)
        data["income"].append(income_lookup.get(str(z), 0))  # ADDED

    nd = pd.DataFrame(data)
    model = st.session_state.model
    proba = model.predict_proba(nd[model_features])[:, 1]
    nd["pred_success"] = proba
    return nd


def generate_candidate_concepts(df_model, max_cuisines=12):
    top_cuisines = (
        df_model["cuisine_main"]
        .value_counts()
        .head(max_cuisines)
        .index
        .tolist()
    )
    # Drop unusable cuisine labels
    top_cuisines = [c for c in top_cuisines if str(c).lower() != "none"]
    if "other" not in top_cuisines:
        top_cuisines.append("other")

    cuisines = sorted(set(top_cuisines))
    price_tiers = [1, 2, 3, 4]
    bool_vals = [True, False]
    alcohol_opts = ["none", "beer_and_wine", "full_bar"]

    concepts = []
    for cuisine in cuisines:
        for price in price_tiers:
            for takeout in bool_vals:
                for delivery in bool_vals:
                    for outdoor in bool_vals:
                        for credit in [True, False]:
                            for has_tv in bool_vals:
                                for alcohol_norm in alcohol_opts:
                                    concepts.append({
                                        "cuisine_main": cuisine,
                                        "price": price,
                                        "takeout": takeout,
                                        "delivery": delivery,
                                        "outdoor": outdoor,
                                        "credit": credit,
                                        "has_tv": has_tv,
                                        "alcohol_norm": alcohol_norm
                                    })
    return concepts


candidate_concepts = generate_candidate_concepts(df_model)


# -----------------------------
# Main mode selection ‚Äì tab-style buttons
# -----------------------------
st.markdown(
    """
    <style>
    div[data-testid="stTabs"] > div {
        background: transparent;
    }
    div[data-testid="stTabs"] [data-baseweb="tab-list"] {
        gap: 12px;
        padding: 8px;
        border-radius: 999px;
        background: #f3f6fb;
        border: 1px solid #dfe7f5;
        justify-content: stretch;
    }
    div[data-testid="stTabs"] [data-baseweb="tab"] {
        flex: 1;
        border-radius: 999px !important;
        padding: 14px 18px !important;
        font-weight: 700;
        font-size: 16px;
        color: #1f2937;
        border: 1px solid transparent;
    }
    div[data-testid="stTabs"] [data-baseweb="tab"]:hover {
        background: #e7eefb;
    }
    div[data-testid="stTabs"] [aria-selected="true"] {
        background: #1e6ffb !important;
        color: #fff !important;
        border-color: #1e6ffb;
        box-shadow: 0 8px 18px rgba(30, 111, 251, 0.18);
    }
    </style>
    <div style="text-align:center; margin: 0 0 0.75rem;">
        <h3 style="margin:0; font-weight:700;">How do you want to use this tool?</h3>
        <p style="margin:6px 0 0; color:#475569;">Tell us what you already know, and we‚Äôll tailor the analysis to your starting point.</p>
    </div>
    """,
    unsafe_allow_html=True,
)

tab_concept, tab_zip = st.tabs(
    ["Mode A ¬∑ Start with a concept", "Mode B ¬∑ Start with a ZIP / neighborhood"]
)

# -----------------------------
# TAB 1: Concept ‚Üí Best ZIPs
# -----------------------------
with tab_concept:
    st.subheader("Mode A: You have a concept ‚Äì we find the best ZIPs")

    with st.expander("Describe your concept", expanded=True):
        cuisines_available = sorted(df_model["cuisine_main"].unique())
        cuisine_choice = st.selectbox("Cuisine", options=cuisines_available)

        price_choice = st.slider("Price tier ($‚Äì$$$$)", 1, 4, 3, step=1)
        takeout_choice = st.checkbox("Offers takeout", True)
        delivery_choice = st.checkbox("Offers delivery", True)
        outdoor_choice = st.checkbox("Outdoor seating", False)
        credit_choice = st.checkbox("Accepts credit cards", True)
        tv_choice = st.checkbox("Has TV", False)
        alcohol_options = {
            "None": "none",
            "Beer and wine": "beer_and_wine",
            "Full bar": "full_bar",
            "Unknown": "unknown"
        }
        alcohol_label = st.selectbox(
            "Alcohol",
            options=list(alcohol_options.keys()),
            index=1
        )
        alcohol_choice = alcohol_options[alcohol_label]

        concept = {
            "cuisine_main": cuisine_choice,
            "price": price_choice,
            "takeout": takeout_choice,
            "delivery": delivery_choice,
            "outdoor": outdoor_choice,
            "credit": credit_choice,
            "has_tv": tv_choice,
            "alcohol_norm": alcohol_choice
        }

    run_concept = st.button("üîç Find the best ZIP codes for this concept")

    if run_concept:
        if not st.session_state.model_trained:
            st.warning("‚ö†Ô∏è Model is still training. Please wait a moment and try again.")
        else:
            # -----------------------------
            # Prediction
            # -----------------------------
            preds = predict_success_for_concept(concept, unique_zips)

            segment_id = segment_id_from_concept(
                cuisine=concept["cuisine_main"],
                price=concept["price"]
            )

            preds["segment_id"] = segment_id
            preds["market_share"] = preds["postal_code"].apply(
                lambda z: lookup_market_share(z, segment_id)
            )
            preds["opportunity"] = preds["pred_success"] * (1 - preds["market_share"])
            preds["cuisine_main"] = concept["cuisine_main"]

            # Drop invalid ZIPs (e.g., nan) before ranking
            preds = preds[
                preds["postal_code"].notna()
                & preds["postal_code"].astype(str).str.lower().ne("nan")
            ].copy()

            preds = preds.merge(
                agg_ddi[["postal_code", "cuisine_main", "ddi"]],
                on=["postal_code", "cuisine_main"],
                how="left"
            )
            preds["ddi"] = preds["ddi"].fillna(0).round(1)

            preds_geo = preds.merge(zip_geo, on="postal_code", how="left")

            # -----------------------------
            # Top table
            # -----------------------------
            st.markdown("### Top 15 ZIPs for this concept (by Opportunity score)")

            top = preds.sort_values("opportunity", ascending=False).head(15)
            top_display = top[["postal_code", "pred_success", "market_share", "opportunity", "ddi"]].copy()

            top_display["pred_success"] = (top_display["pred_success"] * 100).round(1)
            top_display["market_share"] = (top_display["market_share"] * 100).round(1)
            top_display["opportunity"] = (top_display["opportunity"] * 100).round(2)
            top_display["ddi"] = top_display["ddi"].round(1)

            top_display = top_display.rename(columns={
                "postal_code": "ZIP",
                "pred_success": "P(success ‚â•4‚òÖ) [%]",
                "market_share": "Market saturation [%]",
                "opportunity": "Opportunity score",
                "ddi": "DDI (ZIP √ó cuisine)"
            })

            top_display = top_display.reset_index(drop=True)
            top_display.insert(0, "Rank", range(1, len(top_display) + 1))

            st.dataframe(top_display, use_container_width=True, hide_index=True)

            # -----------------------------
            # Interpretation
            # -----------------------------
            with st.expander("How to read these results"):
                st.markdown(
                """
                - **P(success)**  
                This is the model‚Äôs estimate of the chance that **this concept in this ZIP**
                would reach a Yelp rating of **4.0‚òÖ or higher**.  
                It comes from our **random forest** model trained on real Philadelphia restaurant data.

                - **Market saturation**  
                This tells you **how crowded the ZIP already is** with restaurants in the same cuisine segment.  
                - Values near 0 ‚Üí very few direct competitors.  
                - Values near 1 ‚Üí the ZIP is dominated by this cuisine.

                - **Opportunity score**  
                We combine the two ideas above into a single number:  

                `Opportunity score = P(success) √ó (1 ‚àí market saturation)`

                High scores mean the model thinks your concept has a **good chance of hitting ‚â• 4‚òÖ**
                **and** there is still **room in the market** (not overly saturated).

                - **DDI (Demand‚ÄìDissatisfaction Index)**  
                DDI gets large when there are **many reviews** but **low average ratings** for a cuisine in a ZIP.  
                Intuition: lots of people are going out for this type of food (strong demand),
                but they‚Äôre **not fully satisfied** with the current options ‚Äì a sign that a better operator
                might win share.

                - **Why DDI is not included in the opportunity score**  
                Review counts are heavily influenced by **restaurant age**: older places have had more time
                to accumulate reviews, even if demand isn‚Äôt especially strong.  
                If we folded DDI into the Opportunity score, we‚Äôd risk **biasing** the rankings toward
                long-established areas.  
                Instead, DDI is shown only as a **supplementary diagnostic** you can use alongside
                the Opportunity score.
                """
                )
            # -----------------------------
            # Map
            # -----------------------------
            st.markdown("### Map of Opportunity score by ZIP centroid")
            valid_geo = preds_geo.dropna(subset=["lat", "lon"]).copy()

            p = valid_geo["opportunity"]
            if p.max() > p.min():
                norm = (p - p.min()) / (p.max() - p.min())
            else:
                norm = p * 0

            def color_from_norm(x):
                # Smooth gradient: low=red, mid=yellow, high=green
                r = np.interp(x, [0, 0.5, 1], [239, 250, 34]).astype(int)
                g = np.interp(x, [0, 0.5, 1], [68, 204, 197]).astype(int)
                b = np.interp(x, [0, 0.5, 1], [68, 21, 94]).astype(int)
                return [
                    [int(r_i), int(g_i), int(b_i), 200]
                    for r_i, g_i, b_i in zip(r, g, b)
                ]

            valid_geo["color"] = color_from_norm(norm)
            valid_geo["radius"] = 350  # fixed size to avoid dual encoding
            valid_geo["opportunity_pct"] = (valid_geo["opportunity"] * 100).round(2)
            valid_geo = valid_geo.drop(columns=["kind"], errors="ignore")

            layer = pdk.Layer(
                "ScatterplotLayer",
                data=valid_geo,
                get_position='[lon, lat]',
                get_fill_color='color',
                get_radius='radius',
                pickable=True,
                radius_min_pixels=5,
                radius_max_pixels=35,
            )

            tooltip = {
                "html": "<b>ZIP:</b> {postal_code}<br/>"
                        "<b>Opportunity score:</b> {opportunity_pct}%<br/>"
                        "<b>Training venues in ZIP:</b> {n}",
                "style": {"backgroundColor": "white", "color": "black"}
            }

            view_state = pdk.ViewState(
                latitude=float(valid_geo["lat"].mean()),
                longitude=float(valid_geo["lon"].mean()),
                zoom=10
            )

            st.pydeck_chart(pdk.Deck(
                layers=[layer],
                initial_view_state=view_state,
                tooltip=tooltip
            ))

            # Legend
            st.markdown(
                """
                <div style="margin-top:6px;">
                    <div style="display:flex; align-items:center; gap:10px;">
                        <div style="width:200px; height:12px; border-radius:6px; background:linear-gradient(90deg, #ef4444, #facc15 50%, #22c55e);"></div>
                        <span style="font-size:13px; color:#334155;">Opportunity score (lower ‚Üí higher)</span>
                    </div>
                    <div style="font-size:12px; color:#475569; margin-top:4px;">Dot size is fixed; color encodes Opportunity.</div>
                </div>
                """,
                unsafe_allow_html=True
            )

# -----------------------------
# TAB 2: ZIP ‚Üí Best Concepts
# -----------------------------
with tab_zip:
    st.subheader("Mode B: You have a ZIP ‚Äì we find the best concepts")

    zip_choice = st.selectbox("Choose ZIP", options=unique_zips)

    filter_choice_zip = st.radio(
        "Filter concepts",
        options=["Show all", "Food only", "Drink only"],
        horizontal=True
    )

    if not st.session_state.model_trained:
        st.warning("‚ö†Ô∏è Model is still training. Please wait a moment and try again.")
    else:
        if st.button("üîç Find the best concepts for this ZIP"):
            records = []
            for concept in candidate_concepts:
                row = concept.copy()
                row["postal_code"] = zip_choice
                records.append(row)
            cand_df = pd.DataFrame(records)

            # Add income
            cand_df["income"] = income_lookup.get(str(zip_choice), 0)
            cand_df = cand_df[cand_df["cuisine_main"].notna()].copy()
            cand_df = cand_df[
                ~cand_df["cuisine_main"].str.lower().isin(["other", "none"])
            ]
            cand_df["kind"] = cand_df.apply(
                lambda r: classify_food_or_drink(r["cuisine_main"], r["alcohol_norm"]),
                axis=1
            )

            cand_df = apply_food_drink_filter(
                cand_df, kind_column="kind",
                choice="food_only" if filter_choice_zip == "Food only"
                else "drink_only" if filter_choice_zip == "Drink only"
                else "all"
            )
            if cand_df.empty:
                st.info("No concepts remain after applying this filter.")
                st.stop()

            cand_df = cand_df.drop(columns=["kind"], errors="ignore")

            proba = st.session_state.model.predict_proba(cand_df[model_features])[:, 1]
            cand_df["pred_success"] = proba

            cand_df["segment_id"] = cand_df.apply(
                lambda r: segment_id_from_concept(
                    cuisine=r["cuisine_main"],
                    price=r["price"]
                ),
                axis=1
            )

            cand_df["market_share"] = cand_df.apply(
                lambda r: lookup_market_share(zip_choice, r["segment_id"]),
                axis=1
            )

            cand_df["opportunity"] = cand_df["pred_success"] * (1 - cand_df["market_share"])

            cand_df = cand_df.merge(
                agg_ddi[["postal_code", "cuisine_main", "ddi"]],
                on=["postal_code", "cuisine_main"],
                how="left"
            )
            cand_df["ddi"] = cand_df["ddi"].fillna(0).round(1)

            top_concepts = cand_df.sort_values("opportunity", ascending=False).head(15)

            st.markdown(f"### Top 15 concepts for ZIP {zip_choice} (by Opportunity)")
            out = top_concepts[[
                "cuisine_main", "price", "takeout", "delivery", "outdoor",
                "credit", "has_tv", "alcohol_norm",
                "pred_success", "market_share", "opportunity", "ddi"
            ]].copy()

            out["pred_success"] = (out["pred_success"] * 100).round(1)
            out["market_share"] = (out["market_share"] * 100).round(1)
            out["opportunity"] = (out["opportunity"] * 100).round(2)
            out["ddi"] = out["ddi"].round(1)

            out = out.rename(columns={
                "cuisine_main": "Cuisine",
                "price": "Price tier",
                "takeout": "Takeout",
                "delivery": "Delivery",
                "outdoor": "Outdoor",
                "credit": "Cards",
                "has_tv": "TV",
                "alcohol_norm": "Alcohol",
                "pred_success": "P(success ‚â•4‚òÖ) [%]",
                "market_share": "Market saturation in ZIP [%]",
                "opportunity": "Opportunity score",
                "ddi": "DDI (ZIP √ó cuisine)",
                "kind": "Type"
            })

            out["Alcohol"] = out["Alcohol"].fillna("").str.replace("_", " ", regex=False)

            out = out.reset_index(drop=True)
            out.insert(0, "Rank", range(1, len(out) + 1))

            st.dataframe(out, use_container_width=True, hide_index=True)

            with st.expander("How to read these results"):
                st.markdown(
                    f"- This ranking holds ZIP **{zip_choice}** fixed and varies the concept.\n"
                    "- Concepts at the top have **high predicted success** and **low saturation**.\n"
                    "- Market saturation is based on cuisine counts in that ZIP.\n"
                    "- Each row represents one possible concept (cuisine + attributes), ranked by **Opportunity score**.\n"
                )
